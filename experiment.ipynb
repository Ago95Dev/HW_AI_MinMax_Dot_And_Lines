{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Predictive MinMax for Dots and Boxes\n",
    "\n",
    "## Artificial Intelligence 25/26\n",
    "\n",
    "**Obiettivo**: Implementare un agente self-learning per Dots and Boxes usando Predictive MinMax con parametri adattivi L(t) e K(t).\n",
    "\n",
    "### Componenti\n",
    "1. **Game Engine**: Dots and Boxes con griglia 3√ó3\n",
    "2. **MLP (Htrue)**: Multi-Layer Perceptron per valutazione posizioni\n",
    "3. **MinMax**: Con tagli di profondit√† (L) e ampiezza (K)\n",
    "4. **Training Loop**: Self-play ‚Üí Collect data ‚Üí Train MLP\n",
    "5. **Adaptive Strategy**: Diverse strategie per L(t) e K(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our implementations\n",
    "from dots_and_boxes import DotsAndBoxes\n",
    "from mlp_evaluator import MLPEvaluator\n",
    "from minmax import MinMaxAgent\n",
    "from train_loop import TrainingLoop\n",
    "from adaptive_strategy import *\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Game Demonstration\n",
    "\n",
    "Dimostriamo il funzionamento del gioco Dots and Boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 game for quick demonstration\n",
    "game = DotsAndBoxes(grid_size=2)\n",
    "print(\"Initial board:\")\n",
    "print(game.display())\n",
    "print(f\"\\nState vector size: {game.get_state_size()}\")\n",
    "print(f\"Valid moves: {len(game.get_valid_moves())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Strategies Comparison\n",
    "\n",
    "Visualizziamo come evolvono i parametri L e K nelle diverse strategie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all strategies\n",
    "strategies = {\n",
    "    'Progressive': ProgressiveDeepeningStrategy(L_init=1, L_max=5, K_constant=5, step_iterations=10),\n",
    "    'Inverse': InverseRelationshipStrategy(L_init=1, L_max=5, K_init=10, K_min=3, step_iterations=10),\n",
    "    'Exponential': ExponentialGrowthStrategy(L_init=1, L_max=5, K_init=8, K_min=3, growth_rate=0.05),\n",
    "    'Sigmoid': SigmoidStrategy(L_init=1, L_max=5, K_init=10, K_min=3, midpoint=25, steepness=0.15),\n",
    "    'Staircase': StaircaseStrategy(L_schedule=[1,2,3,4,5], K_schedule=[10,8,6,5,4], iterations_per_step=10),\n",
    "    'Constant': ConstantStrategy(L=3, K=5)\n",
    "}\n",
    "\n",
    "# Visualize strategies\n",
    "max_iter = 50\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for name, strategy in strategies.items():\n",
    "    iterations = range(max_iter)\n",
    "    L_values = [strategy.get_params(i)[0] for i in iterations]\n",
    "    K_values = [strategy.get_params(i)[1] for i in iterations]\n",
    "    \n",
    "    ax1.plot(iterations, L_values, label=name, marker='o', markersize=3)\n",
    "    ax2.plot(iterations, K_values, label=name, marker='s', markersize=3)\n",
    "\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('L (Depth)')\n",
    "ax1.set_title('Evolution of Depth Parameter L(t)')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('K (Width)')\n",
    "ax2.set_title('Evolution of Width Parameter K(t)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategies_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Strategies comparison saved to 'strategies_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Experiments\n",
    "\n",
    "Addestriamo l'agente con diverse strategie e confrontiamo i risultati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "GRID_SIZE = 3\n",
    "NUM_ITERATIONS = 40\n",
    "GAMES_PER_ITERATION = 5\n",
    "EPOCHS_PER_BATCH = 2\n",
    "\n",
    "# Select strategies to train (use subset for faster experimentation)\n",
    "strategies_to_train = {\n",
    "    'Progressive': ProgressiveDeepeningStrategy(L_init=1, L_max=4, K_constant=5, step_iterations=8),\n",
    "    'Inverse': InverseRelationshipStrategy(L_init=1, L_max=4, K_init=8, K_min=3, step_iterations=8),\n",
    "    'Sigmoid': SigmoidStrategy(L_init=1, L_max=4, K_init=8, K_min=3, midpoint=20, steepness=0.2),\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Grid size: {GRID_SIZE}x{GRID_SIZE}\")\n",
    "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Games per iteration: {GAMES_PER_ITERATION}\")\n",
    "print(f\"  Strategies to train: {list(strategies_to_train.keys())}\")\n",
    "print(f\"\\nEstimated time: ~{NUM_ITERATIONS * GAMES_PER_ITERATION * len(strategies_to_train) * 2} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with each strategy\n",
    "results = {}\n",
    "\n",
    "for strategy_name, strategy in strategies_to_train.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with {strategy_name} Strategy\")\n",
    "    print(f\"{strategy.get_description()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create training loop\n",
    "    trainer = TrainingLoop(grid_size=GRID_SIZE, hidden_sizes=[128, 64])\n",
    "    \n",
    "    # Train\n",
    "    history = []\n",
    "    for iteration in tqdm(range(NUM_ITERATIONS), desc=f\"{strategy_name}\"):\n",
    "        L, K = strategy.get_params(iteration)\n",
    "        \n",
    "        stats = trainer.train_iteration(\n",
    "            L=L, \n",
    "            K=K, \n",
    "            num_games=GAMES_PER_ITERATION,\n",
    "            epochs_per_batch=EPOCHS_PER_BATCH,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        history.append(stats)\n",
    "    \n",
    "    results[strategy_name] = {\n",
    "        'trainer': trainer,\n",
    "        'strategy': strategy,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    # Print final summary\n",
    "    final_loss = trainer.mlp.get_average_recent_loss(n=10)\n",
    "    print(f\"\\n{strategy_name} - Final average loss: {final_loss:.4f}\")\n",
    "    print(f\"Total games played: {trainer.games_played}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n",
    "\n",
    "Analizziamo e confrontiamo le performance delle diverse strategie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss over time\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for strategy_name, data in results.items():\n",
    "    history = data['history']\n",
    "    iterations = [h['iteration'] for h in history]\n",
    "    losses = [h['loss'] for h in history]\n",
    "    \n",
    "    plt.plot(iterations, losses, label=strategy_name, marker='o', markersize=4, linewidth=2)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Training Loss Comparison Across Strategies', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_loss_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training loss plot saved to 'training_loss_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for strategy_name, data in results.items():\n",
    "    history = data['history']\n",
    "    trainer = data['trainer']\n",
    "    \n",
    "    # Get final metrics\n",
    "    final_loss = history[-1]['loss']\n",
    "    avg_recent_loss = trainer.mlp.get_average_recent_loss(n=10)\n",
    "    total_games = trainer.games_played\n",
    "    \n",
    "    # Calculate loss improvement\n",
    "    initial_loss = history[0]['loss']\n",
    "    improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Strategy': strategy_name,\n",
    "        'Final Loss': f\"{final_loss:.4f}\",\n",
    "        'Avg Recent Loss': f\"{avg_recent_loss:.4f}\",\n",
    "        'Improvement': f\"{improvement:.1f}%\",\n",
    "        'Total Games': total_games\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Game Outcome Analysis\n",
    "\n",
    "Analizziamo la distribuzione degli esiti delle partite nel tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot game outcomes over time for best strategy\n",
    "best_strategy_name = min(results.keys(), key=lambda k: results[k]['history'][-1]['loss'])\n",
    "print(f\"Best performing strategy: {best_strategy_name}\\n\")\n",
    "\n",
    "history = results[best_strategy_name]['history']\n",
    "\n",
    "# Extract outcomes\n",
    "iterations = []\n",
    "player1_wins = []\n",
    "player2_wins = []\n",
    "ties = []\n",
    "\n",
    "for h in history:\n",
    "    iterations.append(h['iteration'])\n",
    "    outcomes = h['outcomes']\n",
    "    total = sum(outcomes.values())\n",
    "    player1_wins.append(outcomes[1] / total * 100)\n",
    "    player2_wins.append(outcomes[-1] / total * 100)\n",
    "    ties.append(outcomes[0] / total * 100)\n",
    "\n",
    "# Create stacked area plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.stackplot(iterations, player1_wins, ties, player2_wins, \n",
    "              labels=['Player 1 Wins', 'Ties', 'Player 2 Wins'],\n",
    "              colors=['#2ecc71', '#95a5a6', '#e74c3c'],\n",
    "              alpha=0.7)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Percentage (%)', fontsize=12)\n",
    "plt.title(f'Game Outcomes Distribution - {best_strategy_name} Strategy', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.savefig('game_outcomes_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Game outcomes plot saved to 'game_outcomes_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MLP Behavior Visualization\n",
    "\n",
    "Vediamo come l'MLP valuta diverse posizioni prima e dopo il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MLP evaluation before and after training\n",
    "best_trainer = results[best_strategy_name]['trainer']\n",
    "\n",
    "# Generate sample positions\n",
    "sample_games = []\n",
    "for _ in range(20):\n",
    "    game = DotsAndBoxes(grid_size=GRID_SIZE)\n",
    "    # Make random moves\n",
    "    num_moves = np.random.randint(0, min(10, len(game.get_valid_moves())))\n",
    "    for _ in range(num_moves):\n",
    "        if not game.is_game_over():\n",
    "            moves = game.get_valid_moves()\n",
    "            if moves:\n",
    "                game.make_move(np.random.choice(moves))\n",
    "    sample_games.append(game)\n",
    "\n",
    "# Evaluate with trained MLP\n",
    "trained_evals = [best_trainer.mlp.evaluate_state(g.get_state_vector()) for g in sample_games]\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(trained_evals, bins=20, edgecolor='black', alpha=0.7, color='#3498db')\n",
    "plt.xlabel('MLP Evaluation', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title(f'Distribution of MLP Evaluations - {best_strategy_name} Strategy', fontsize=14, fontweight='bold')\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Neutral (0)')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mlp_evaluation_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"MLP Evaluation Statistics:\")\n",
    "print(f\"  Mean: {np.mean(trained_evals):.4f}\")\n",
    "print(f\"  Std:  {np.std(trained_evals):.4f}\")\n",
    "print(f\"  Min:  {np.min(trained_evals):.4f}\")\n",
    "print(f\"  Max:  {np.max(trained_evals):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "1. **Convergence**: Tutte le strategie mostrano una riduzione della loss nel tempo, dimostrando che il self-play funziona.\n",
    "\n",
    "2. **Strategy Comparison**: \n",
    "   - Le strategie con crescita graduale (Progressive, Sigmoid) tendono ad essere pi√π stabili\n",
    "   - La strategia Inverse mostra buone performance bilanciando esplorazione e sfruttamento\n",
    "\n",
    "3. **Learning Behavior**: \n",
    "   - Nelle prime iterazioni, l'MLP impara velocemente (loss diminuisce rapidamente)\n",
    "   - Dopo ~20-30 iterazioni, le performance si stabilizzano\n",
    "\n",
    "4. **Adaptive Parameters**:\n",
    "   - Aumentare L (profondit√†) migliora la qualit√† della ricerca\n",
    "   - Diminuire K (ampiezza) riduce il costo computazionale senza penalizzare troppo le performance\n",
    "\n",
    "### Best Strategy\n",
    "\n",
    "Based on the experiments, the **{best_strategy_name}** strategy achieved the best results with:\n",
    "- Lowest final loss\n",
    "- Stable training progression\n",
    "- Good balance between exploration and exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_trainer.save_checkpoint(f'best_model_{best_strategy_name.lower()}.pth')\n",
    "print(f\"Best model saved: best_model_{best_strategy_name.lower()}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optional: Test Trained Agent\n",
    "\n",
    "Proviamo l'agente addestrato contro un agente casuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minmax import RandomAgent\n",
    "\n",
    "# Play a test game\n",
    "game = DotsAndBoxes(grid_size=GRID_SIZE)\n",
    "trained_agent = MinMaxAgent(best_trainer.mlp, L=3, K=4)\n",
    "random_agent = RandomAgent()\n",
    "\n",
    "print(\"Trained Agent (Player 1) vs Random Agent (Player 2)\\n\")\n",
    "print(game.display())\n",
    "\n",
    "move_count = 0\n",
    "while not game.is_game_over() and move_count < 30:\n",
    "    if game.current_player == 1:\n",
    "        move = trained_agent.select_move(game)\n",
    "        print(f\"\\nTrained agent plays: {move}\")\n",
    "    else:\n",
    "        move = random_agent.select_move(game)\n",
    "        print(f\"\\nRandom agent plays: {move}\")\n",
    "    \n",
    "    game.make_move(move)\n",
    "    move_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL BOARD:\")\n",
    "print(\"=\"*60)\n",
    "print(game.display())\n",
    "\n",
    "winner = game.get_winner()\n",
    "if winner == 1:\n",
    "    print(\"\\nüéâ Trained agent WINS!\")\n",
    "elif winner == -1:\n",
    "    print(\"\\nüòû Random agent wins\")\n",
    "else:\n",
    "    print(\"\\nü§ù It's a TIE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
